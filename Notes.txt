Art of Problem-Solving is first need to understand problem which means understanding what we want to solve what we want to achive what we have(input and output). Then thinking about how I can go from input to output logically naturally and if possible need to make it mathematical problem and then coding and then optimizing problem in such way that time taken from input to output is minimal

- If you want to execute loop only based on condition rather then iteration consider using while loop.
- Big O say how well your code solve problem. how well it performs when we scale it. 

What is good code?
	1. Readable - How well your code can be read.
	2. Scalable - big O - How well code perform when we scale the input

# Big O

- Big O is measurement to see how well code performs when we increases input.
- Big O means how much number of operation increases of function or algorithm when we increase input.

## so how we represent big O in practical

- it is O(n) here n represent number of input. and O(n) represent number of operation on given n.


## What are they 

O(n) -  linear time complexity. why linear? it says with n number of input it will take n number of operation.

Here is one special that I love most 

O(1) - Constant time complexity. why constant? it says with any number of input it will take constant operation. so number of input will not at all affect 	the number of operation 

	for example
		number of input = 1 - number of operation = 1
		number o input = 10 - number of operation = 1
		number o input = 10k - number of operation = 1

	Most scalable algorithm. that is the goal of any algorithm development
	


## How we actually calculate big O

- First thing we only care how algorithm behaves as input increase. For example it increase linearly, constantly etc..
- As I have noticed there are multiple line in code and every line has it's operation. So we want to check how many number of operation that particular line needs to do on given input. 


def printHello(input):
	a = 10 # O(1)
	b = 100 # O(1)
	for i in input: # O(n)
		print(hello) # O(n)
	return # O(1)

	
Total time complexity is 3 + 2n = O(3 + 2n) = O(n)

## Rule book

Rule 1: Always worst case
Rule 2: Remove constant
Rule 3: Different input should have different variable O(a+b) or O(a*b)
Rule 4: Drop non-dominant term

======================================================================
No matter how big the constant is and how slow the linear increase is, linear will at some point surpass constant.

If your algorithm is in the form "do this, then, when you're all done, do that" then you add the runtimes. 
If your algorithm is in the form "do this for each time you do that" then you multiply the runtimes. 


## Amortized time
	Amortized time is the average time an operation takes, considering that some rare, slow operations are spread out over many fast ones. means we ignore rare slow operation.

### Log N

	Every step you cut down input size by half, binery search

	16 = 2^4
	log 16 = 4


	Log N means what power of 2 makes N. 



Data structure +  Algorithm = Program

good + good =  good

for that we have to pick right data structure and right algorithm and to right program. the only matters is how scalable solution is how it performs when we scale.


Most expensive time complexity is n!, never every your code should have this time complexity


O(N!) = Factorial - I am adding a loop for every element in input.




What is best code?

Three pillars of good code. Good engineer is very careful of below 3 pillars. they define engineer.

1. readable - It should be readable
2. speed - It should be faster
3. Space - It should not consume lot of memory


What causes space complexity

1. Variables
2. function calls
3. allocation
4. Data structure



When we talk about space complexity we talk about additioanal space not initial space taken by input.



Data Structures
==================================================================================================================================
There some operation I can perform on data structure

- insertion
- Deletion
- Traversal
- searching
- sorting
- Access

-- Each data structure has different pros and cons for this operation. Some data structure has good at certain operation some at bad. we are doing different tradeoff between this data structure

Let's start our journey to data structure

1. Array
=================================================
Array is most basic data structure and most used

Access, add, and pop operation are most efficient with O(1) - constant

while insert, delete and traversing operation are O(n) - linear

There are two types of array static Array and Dynamic Array. 

In static Array we are assigning fixed size of memory block which we initially created and we not grow from that

In dynamic Array we usually assign double of memory of initial array size. and If we reach end of memory higher level language usually again create doubled memory of size array and relocate array. 


So, sometime append operation take O(n) time due to this and we usually don't consider this because rule of amortize where we ignore some case that happen once a while.


-- Always consider String as Array. and solve problem. So, First convert String to Array solve it and make string.

-- Python and JavaScript string can be access as Array

-- Always deliver most sophisticated modern way solution to a problem and talk through it.

-- Always explain pros and cons of solution, that's sign of great engineer.

=============================================================================================================================================================
Sorting Alogorithms
=============================================================================================================================================================
How we classify algorithm:
	1. Time complexity
	2. Space complexity
		- In-place or constant Time
		- Space grows with input size
	3. Stability
		- If there are two elements with same value if algorithm preserve original size we can say it is stable algorithm
	4. Internal vs Extranl 
		- Internal: Use is RAM only or Records are on RAM
		- External: Use Disk or Records are on disk
	5. Recursive or Non-Recusrsive
--------------------------------------------------------------------------------------------------------------------------
Selection Sorting - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
Selection is sort is slow sorting algorithm it takes O(n^2) time complexity

How Selection sort works:
	- In Selection sort at each iteration we select element and comapare with other elements in list and put the minimal element in ascdeing order for descnding we will do reverse.  
	- The other version of selection sort where we keep other array for sorted elements Space complexity will be O(n) with same O(n^2) time complexity

1. Time complexity - Worst Case: O(n^2); Avg Case: O(n^2); Best Case: O(n^2)
2. Space complexity - O(1) In-place
3. Stability - Yes
4. Internal - Yes
5. Non-Recusrsive
--------------------------------------------------------------------------------------------------------------------------
Bubble Sorting - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
sort is slow sorting algorithm it takes O(n^2) time complexity.

How Bubble sort works:
	- At each iteration we compare adjusant elements and swap them in sorted order at on full cycle of inner for loop the largest element will be bubble up.
	- Here we scan list of elements till all are sorted and each scan largest wil be bubbled up to last, so every time n to n-i element will be sorted.
	- For beat case if we use flag time complexity will be O(n);

1. Time complexity - Worst Case: O(n^2); Avg Case: O(n^2); Best Case: O(n)
2. Space complexity - O(1) In-place
3. Stability - Yes
4. Internal - Yes
5. Non-Recusrsive
--------------------------------------------------------------------------------------------------------------------------
Insertion Sorting - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
Insertion sort is slow sorting algorithm it takes O(n^2) time complexity. But for best case Time Complexity: O(n); and also if we see practically logic of insertion sort it has less comprasaion and loop so it is quite better then selection and bubble sort.

How Insertion sort works:
    - First we take list of array and we assume left side is sorted,
    - We take each element from begining and make hole at the index of element.
	- we compare with previous (left side sorted array) then we shift larger element in hole and make hole and repeat till we find appropiat hole for our current or targeted element. and insert.

1. Time complexity - Worst Case: O(n^2); Avg Case: O(n^2); Best Case: O(n)
2. Space complexity - O(1) In-place
3. Stability - Yes
4. Internal - Yes
5. Non-Recusrsive

--------------------------------------------------------------------------------------------------------------------------
Merge Sorting - O(nlog(n)) time complexity; O(n) space complexity
--------------------------------------------------------------------------------------------------------------------------
Merge sort is good algorithmn as time complexity is O(nlog(n)) but space complexity is O(n).
- Merge sort is recursive algorithm as it solve problem is self similar manner. and it follows divide and conqur principle to solve problem.

How it works:
    - First we take input as array and we divide it in left and right part with middle index and we divide till we get to the single element and single element is always sorted. now we start merging element for that we take those left and right element which are already sorted and we merge them in sorting order.

1. Time complexity - Worst Case: O(nlog(n));
2. Space complexity - O(n)(Recusrsion Call + Additional Array Space).
3. Stability - Yes
4. Internal - Yes
5. Recusrsive
6. Divide and Conqur principle
7. In-Place - No

--------------------------------------------------------------------------------------------------------------------------
Quick Sorting - O(nlog(n)) time complexity; O(log(n)) space complexity
--------------------------------------------------------------------------------------------------------------------------
Quick sort is most efficiant algorithm as it take O(nlog(n)) time and O(log(n)) space complexity 
- We can avoid worst case by selecting pivot element as random element and then set it last element.
- Quick Sort is In-Place sorting algorithm we don't take extra memory to store array value but recursion take log(n) space in stack memory.

How quick Sort work:
    - First quick sort take array as input and start and end index then first it find partition so we divide array with left side values are always less then pivot element(Last element(we randomly select from array and swap with last element)). And right side always greater then pivot.
    - We repeat this pprocess self similar manner means recursivly calling quick sort again with left side and right side of Pivot element.
    - So it takes n times for partion and log(n) time for calling quick sort again with half element.

1. Time complexity - Best and Avg Case: O(nlog(n)); Worst Case: O(n^2);
2. Space complexity - Best and Avg Case: O(log(n))(Recursive call Stack Space); Worst Case: O(n);
3. Stability - Yes
4. Internal - Yes
5. Recusrsive
6. Divide and Conqur principle
7. In-Place - Yes