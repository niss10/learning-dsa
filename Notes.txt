Art of Problem-Solving is first need to understand problem which means understanding what we want to solve what we want to achive what we have(input and output). Then thinking about how I can go from input to output logically naturally and if possible need to make it mathematical problem and then coding and then optimizing problem in such way that time taken from input to output is minimal

- If you want to execute loop only based on condition rather then iteration consider using while loop.
- Big O say how well your code solve problem. how well it performs when we scale it. 

What is good code?
	1. Readable - How well your code can be read.
	2. Scalable - big O - How well code perform when we scale the input

# Big O

- Big O is measurement to see how well code performs when we increases input.
- Big O means how much number of operation increases of function or algorithm when we increase input.

## so how we represent big O in practical

- it is O(n) here n represent number of input. and O(n) represent number of operation on given n.


## What are they 

O(n) -  linear time complexity. why linear? it says with n number of input it will take n number of operation.

Here is one special that I love most 

O(1) - Constant time complexity. why constant? it says with any number of input it will take constant operation. so number of input will not at all affect 	the number of operation 

	for example
		number of input = 1 - number of operation = 1
		number o input = 10 - number of operation = 1
		number o input = 10k - number of operation = 1

	Most scalable algorithm. that is the goal of any algorithm development
	


## How we actually calculate big O

- First thing we only care how algorithm behaves as input increase. For example it increase linearly, constantly etc..
- As I have noticed there are multiple line in code and every line has it's operation. So we want to check how many number of operation that particular line needs to do on given input. 


def printHello(input):
	a = 10 # O(1)
	b = 100 # O(1)
	for i in input: # O(n)
		print(hello) # O(n)
	return # O(1)

	
Total time complexity is 3 + 2n = O(3 + 2n) = O(n)

## Rule book

Rule 1: Always worst case
Rule 2: Remove constant
Rule 3: Different input should have different variable O(a+b) or O(a*b)
Rule 4: Drop non-dominant term

======================================================================
No matter how big the constant is and how slow the linear increase is, linear will at some point surpass constant.

If your algorithm is in the form "do this, then, when you're all done, do that" then you add the runtimes. 
If your algorithm is in the form "do this for each time you do that" then you multiply the runtimes. 


## Amortized time
	Amortized time is the average time an operation takes, considering that some rare, slow operations are spread out over many fast ones. means we ignore rare slow operation.

### Log N

	Every step you cut down input size by half, binery search

	16 = 2^4
	log 16 = 4


	Log N means what power of 2 makes N. 



Data structure +  Algorithm = Program

good + good =  good

for that we have to pick right data structure and right algorithm and to right program. the only matters is how scalable solution is how it performs when we scale.


Most expensive time complexity is n!, never every your code should have this time complexity


O(N!) = Factorial - I am adding a loop for every element in input.




What is best code?

Three pillars of good code. Good engineer is very careful of below 3 pillars. they define engineer.

1. readable - It should be readable
2. speed - It should be faster
3. Space - It should not consume lot of memory


What causes space complexity

1. Variables
2. function calls
3. allocation
4. Data structure



When we talk about space complexity we talk about additioanal space not initial space taken by input.



Data Structures
==================================================================================================================================
There some operation I can perform on data structure

- insertion
- Deletion
- Traversal
- searching
- sorting
- Access

-- Each data structure has different pros and cons for this operation. Some data structure has good at certain operation some at bad. we are doing different tradeoff between this data structure

Let's start our journey to data structure

1. Array
=================================================
Array is most basic data structure and most used

Access, add, and pop operation are most efficient with O(1) - constant

while insert, delete and traversing operation are O(n) - linear

There are two types of array static Array and Dynamic Array. 

In static Array we are assigning fixed size of memory block which we initially created and we not grow from that

In dynamic Array we usually assign double of memory of initial array size. and If we reach end of memory higher level language usually again create doubled memory of size array and relocate array. 


So, sometime append operation take O(n) time due to this and we usually don't consider this because rule of amortize where we ignore some case that happen once a while.


-- Always consider String as Array. and solve problem. So, First convert String to Array solve it and make string.

-- Python and JavaScript string can be access as Array

-- Always deliver most sophisticated modern way solution to a problem and talk through it.

-- Always explain pros and cons of solution, that's sign of great engineer.

=============================================================================================================================================================
Sorting Alogorithms
=============================================================================================================================================================
How we classify algorithm:
	1. Time complexity
	2. Space complexity
		- In-place or constant Time
		- Space grows with input size
	3. Stability
		- If there are two elements with same value if algorithm preserve original size we can say it is stable algorithm
	4. Internal vs Extranl 
		- Internal: Use is RAM only or Records are on RAM
		- External: Use Disk or Records are on disk
	5. Recursive or Non-Recusrsive
--------------------------------------------------------------------------------------------------------------------------
Selection Sorting - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
Selection is sort is slow sorting algorithm it takes O(n^2) time complexity

How Selection sort works:
	- In Selection sort at each iteration we select element and comapare with other elements in list and put the minimal element in ascdeing order for descnding we will do reverse.  
	- The other version of selection sort where we keep other array for sorted elements Space complexity will be O(n) with same O(n^2) time complexity

1. Time complexity - Worst Case: O(n^2); Avg Case: O(n^2); Best Case: O(n^2)
2. Space complexity - O(1) In-place
3. Stability - Yes
4. Internal - Yes
5. Non-Recusrsive
--------------------------------------------------------------------------------------------------------------------------
Bubble Sorting - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
sort is slow sorting algorithm it takes O(n^2) time complexity.

How Bubble sort works:
	- At each iteration we compare adjusant elements and swap them in sorted order at on full cycle of inner for loop the largest element will be bubble up.
	- Here we scan list of elements till all are sorted and each scan largest wil be bubbled up to last, so every time n to n-i element will be sorted.
	- For beat case if we use flag time complexity will be O(n);

1. Time complexity - Worst Case: O(n^2); Avg Case: O(n^2); Best Case: O(n)
2. Space complexity - O(1) In-place
3. Stability - Yes
4. Internal - Yes
5. Non-Recusrsive
--------------------------------------------------------------------------------------------------------------------------
Insertion Sorting - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
Insertion sort is slow sorting algorithm it takes O(n^2) time complexity. But for best case Time Complexity: O(n); and also if we see practically logic of insertion sort it has less comprasaion and loop so it is quite better then selection and bubble sort.

How Insertion sort works:
    - First we take list of array and we assume left side is sorted,
    - We take each element from begining and make hole at the index of element.
	- we compare with previous (left side sorted array) then we shift larger element in hole and make hole and repeat till we find appropiat hole for our current or targeted element. and insert.

1. Time complexity - Worst Case: O(n^2); Avg Case: O(n^2); Best Case: O(n)
2. Space complexity - O(1) In-place
3. Stability - Yes
4. Internal - Yes
5. Non-Recusrsive

--------------------------------------------------------------------------------------------------------------------------
Merge Sorting - O(nlog(n)) time complexity; O(n) space complexity
--------------------------------------------------------------------------------------------------------------------------
Merge sort is good algorithmn as time complexity is O(nlog(n)) but space complexity is O(n).
- Merge sort is recursive algorithm as it solve problem is self similar manner. and it follows divide and conqur principle to solve problem.

How it works:
    - First we take input as array and we divide it in left and right part with middle index and we divide till we get to the single element and single element is always sorted. now we start merging element for that we take those left and right element which are already sorted and we merge them in sorting order.

1. Time complexity - Worst Case: O(nlog(n));
2. Space complexity - O(n)(Recusrsion Call + Additional Array Space).
3. Stability - Yes
4. Internal - Yes
5. Recusrsive
6. Divide and Conqur principle
7. In-Place - No

--------------------------------------------------------------------------------------------------------------------------
Quick Sorting - O(nlog(n)) time complexity; O(log(n)) space complexity
--------------------------------------------------------------------------------------------------------------------------
Quick sort is most efficiant algorithm as it take O(nlog(n)) time and O(log(n)) space complexity 
- We can avoid worst case by selecting pivot element as random element and then set it last element.
- Quick Sort is In-Place sorting algorithm we don't take extra memory to store array value but recursion take log(n) space in stack memory.

How quick Sort work:
    - First quick sort take array as input and start and end index then first it find partition so we divide array with left side values are always less then pivot element(Last element(we randomly select from array and swap with last element)). And right side always greater then pivot.
    - We repeat this pprocess self similar manner means recursivly calling quick sort again with left side and right side of Pivot element.
    - So it takes n times for partion and log(n) time for calling quick sort again with half element.

1. Time complexity - Best and Avg Case: O(nlog(n)); Worst Case: O(n^2);
2. Space complexity - Best and Avg Case: O(log(n))(Recursive call Stack Space); Worst Case: O(n);
3. Stability - Yes
4. Internal - Yes
5. Recusrsive
6. Divide and Conqur principle
7. In-Place - Yes

=============================================================================================================================================================
Searching Alogorithms
=============================================================================================================================================================
In this below notes we will discuss various type of Searching Algorithm.
--------------------------------------------------------------------------------------------------------------------------
Linear Search - O(n^2) time complexity; O(1) space complexity
--------------------------------------------------------------------------------------------------------------------------
Time Complexity = O(n); Space Complexity = O(1);

Self explanatatoy we do linearly check one by one elment and match with given value if find we will return index;

--------------------------------------------------------------------------------------------------------------------------
Binary Search - O(log(n)) time complexity; O(1) space complexity(Iterative Implementation) O(log(n))(Recusrsive Implementation)
--------------------------------------------------------------------------------------------------------------------------
When ever you are given with sorted array and you need to solve problem by searching somthing think of Binary Search.
// Time Complexity = O(nlog(n)); Space Complexity = O(log(n));
Note: Binary search itself take log(n) time complexity. But there is added time for Sorting Input.
It must have sorted input to work upon;
Binary Search work upon three case.
Case - 1: Every time we check middle element and if target value match middle element we return found and index.
Case - 2: If target is less then middle element in next iterative or recursive call we will check array elements from start till middle.
Case - 3: If target is greater then middle element in next iterative or recursive call we will check array elements from middle to end.

We will repeat this process till found element or start index becomes greater then end.
So there will be two exit condition 1. If we found 2. IF we check all element and not found this case start > end;

--------------------------------------------------------------------------------------------------------------------------
Binary Search First Occurance of Element - O(log(n)) time complexity; O(1) space complexity(Iterative Implementation)
--------------------------------------------------------------------------------------------------------------------------
- Binary search have many variation and can be applied in many problem and one problem is when we want to find first occurance of element in sorted array.

- to solve this problem we will not stop searching element when we find first time element instead if we will search again on left side and update our result inde every time we find lets see code everything will be clear seeing code.

--------------------------------------------------------------------------------------------------------------------------
Binary Search Last Occurance of Element - O(log(n)) time complexity; O(1) space complexity(Iterative Implementation)
--------------------------------------------------------------------------------------------------------------------------
- Binary search have many variation and can be applied in many problem and one problem is when we want to find last occurance of element in sorted array.

- to solve this problem we will not stop searching element when we find first time element instead if we will search again on this time right side and update our result inde every time we find lets see code everything will be clear seeing code.

--------------------------------------------------------------------------------------------------------------------------
Binary Search find total occurrence of Element - O(log(n)) time complexity; O(1) space complexity(Iterative Implementation)
--------------------------------------------------------------------------------------------------------------------------
- To find total occurrence we need to first calculate first occurance of element and last occurrence then  we can get length of by subtracting last occurrence - first occurrence as all occurance is between first occurrence and last occurrence.

--------------------------------------------------------------------------------------------------------------------------
Binary Search find total rotation of sorted array  - O(log(n)) time complexity; O(1) space complexity(Iterative Implementation)
--------------------------------------------------------------------------------------------------------------------------
- To find total occurrence we need to first calculate first occurance of element and last occurrence then  we can get length of by subtracting last occurrence - first occurrence as all occurance is between first occurrence and last occurrence.

 - This works on circularly sorted array and there must be no duplicate in array.
 - To solve this problem we can use binear search variation here.
 First we check if start element and last element if start element is smaller then last element we can say array is already sorted on it is not rotated.
 - Okay so now if let's assume array is rotated one or more then one times. 
 Then we need to find smallest element in array if previous element and next element is greater then current element we can say it is smallest element and all elements before are circularly shifted so index of that element is count of rotation.
 - Now to loop an find this condition we check start elment and mid element if start is less then mid element we can sure that all element before that mid element are sorted then we need to loop through right side of element. 
 - Else is start elment is greater then mid element we can say that our target element is in that portion start to mid.
 - So here every time we are dividing array in two parts. We never check sorted part. See the code you will get idea.

--------------------------------------------------------------------------------------------------------------------------
Find element in circularly sorted array  - O(log(n)) time complexity; O(1) space complexity(Iterative Implementation)
--------------------------------------------------------------------------------------------------------------------------
 - Here we are finding element in circularly rotated Array. To do this operation list must not have any duplicate element.
- Okay, now we do same as binary search there will be three case.

Case 1: we find mid index and compare element with target. if match we return index;
Case 2: Target Greater then mid element;
    - now element can be either left side or right side. And we are sure that one of the side will always be sorted(this is the key to find where we can go)
    Case 2A: if(mid element is less then end element this side is sorted)
        - now if target element is greater then mid element and less then end element now we can search in this segment
        - Else element is on left side.
    Case 2B: if we reach here means right side of mid element is not sorted so we check here
        - if element is in left side(sorted). if target is greater then start element and less then mid element
        - Else element is on right side.   
Case 3: Target is less then mid element;
    Case 3A: if(start element is less then mid element and target is also with in them)
        - We check if tartget is in left sorted part
        - Else it is on right side
    Case 3B: else target is right side but right side is sorted
        - we check if it in right side 
        - else it is on left side

See the code closely you will get idea it is complex yet very simple.